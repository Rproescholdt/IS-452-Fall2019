{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions\n",
    "\n",
    "For the past two weeks we've been learning how to extract text out of XML documents via XPath statements.  Now we're going to turn our attention to pure text.  So whereas XPath statements describe patterns of locations in XML documents, regular expressions describe how text might look.\n",
    "\n",
    "For example, if we're looking for instances in a document where a year is noted (e.g. 2017), we could search for the year we expected.  But what if there a range of years?  We could search for \"201\" to catch 2010-2019, but we'd need to switch patterns for anything outside that range.  Likewise, if you're looking for a year anywhere between 1900-1999 you could search for just \"19\" but now you might get ages, days, or other numerical values.\n",
    "\n",
    "This kind of search is exactly what regular expressions are designed to do.  Instead of starting with constituant numbers, we could build up a more specific pattern of what that year might look like.  \n",
    "\n",
    "For the range: 2010-2019:\n",
    "\n",
    "* We know that a year is composed of four integer values all together. Yes, sometimes years might be in \"'nn\" or \"nn\" format, but let's say there will be 4 numbers in the document we're searching on.\n",
    "* We know that the first three numbers will be constant:  a literal 2, a literal 0, and a literal 1.  \n",
    "* The last number is what can vary, and we can say it is any integer value between 0 and 9.\n",
    "\n",
    "These three rules can be described within a regular expression. \n",
    "\n",
    "This is a good time to skip off and do some reading before continuing.  You should be reading chapter 11 of PfE this week, but there's another source you can look for.  Our friendly relevant Wikipedia page.  https://en.wikipedia.org/wiki/Regular_expression  You can skim some of the narrative for background, but I'd like you to do more focused reading on:\n",
    "\n",
    "* Basic concepts: https://en.wikipedia.org/wiki/Regular_expression#Basic_concepts\n",
    "* Formal langugae: https://en.wikipedia.org/wiki/Regular_expression#Formal_language_theory\n",
    "    * Gloss over the math notation, focus on the examples and context\n",
    "* Syntax:  https://en.wikipedia.org/wiki/Regular_expression#Syntax\n",
    "    * Head for that table and focus on reading the examples rather than understanding the technical jargon.\n",
    "    \n",
    "    \n",
    "Many software programs dealing with text have support for regular expression searches.  Even PyCharm!  When you open up a text file, start a search inside of it, and you'll see a check box for Regex.  So you can practice on this without needing to use Python.\n",
    "\n",
    "In fact, that's what I recommend.  Particularly when you're trying to do known item searchers within a text document, you often want to iteratively experiment with your expressions directly on the document before you bring it into python to extract those results.  This will give you instant feedback of what it is finding.\n",
    "\n",
    "For example, say you have a document with 100 records.  The data is semi-structured, so you've decided to use a regular expression to extract out a certain data point.  As your query, there will be a result count.  When you think you have your expression done, check the count.  If you see something other than 100, you know that you need to change it.  \n",
    "\n",
    "* A number less than your known result count means that you've made your expression too restrictive.  You're falsely rejecting some data.  \n",
    "* A number more than your known result count means that you've made your expression too permissive.  You're flasely accepting some data.\n",
    "\n",
    "There will be times that you cannot get exactly what you need with a single regular expression.  That's usually because the data is too unstructured and the rules are too complex or broad to be applied over the entire document.  This is usually a good point, and a valuable place, to open your string processing skills to subdivide the document.  For example, if you have a very broad search, you might want to do make subdivisions (remember splitting Dracula apart?) and apply the broad search to just the sections that you know apply.\n",
    "\n",
    "For example, say you have a long report of 100,000 summary records in one document.  These are records on snake species and their field measurements.  You want the length field, which is present within each record, but you only want it for the boa species.  This is a pretty classic data query, and maybe you can imagine how easy it would be to construct in SQL.  But instead of a lovely database, you have an unlovely semistructured text report.\n",
    "\n",
    "So trying to write a regular expression to get the length value is going to be overly permissive.  You're going to get it for all 100,000 records.  But say you know that there are 45,000 boa records.  You know this because you've also run a regular expression to detect how many species records are classified as boa.  It might be possible to include that subdivision in your regular expression, but there's a good chance it'll be so complex and unweidly that you won't be able to contain it or make a good slice.  But, you very likely could split the document apart, such that you have all 100,000 records as separate strings, then you can filter out just the boa records, and then apply your length expression to just that subset of strings.  \n",
    "\n",
    "This kind of situation is when using regular expressions in the context of Python is very valuable.  \n",
    "\n",
    "There are also situations when using other mixed methods, such as a combination of regular expressions and xpath statements make a lot of sense.  For example, when doing web scraping, sometimes there will be fields contained in single HTML elements that actually have multiple data points.  Those data points are only separated by text delimiters (words or symbols).  So you use xpath to cleanly extract the element text value, and then you throw that text into regular expressions for splitting.  This is another example of subsetting known data before feeding it into regular expressions.  The more you can clean away noise from your source text before applying a regular expression, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions in PyCharm\n",
    "\n",
    "Let's practice a few expressions on our ab-104.xml document.  Yes, this is an xml document, but it is just plain text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
